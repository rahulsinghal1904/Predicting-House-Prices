{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":3,"outputs":[{"output_type":"stream","text":"/kaggle/input/house-data/objects.bin\n/kaggle/input/house-data/m_1ce96d9d245ca490.0000\n/kaggle/input/house-data/m_1ce96d9d245ca490.sidx\n/kaggle/input/house-data/dir_archive.ini\n/kaggle/input/house-data/m_1ce96d9d245ca490.frame_idx\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install turicreate\nimport turicreate as tc","execution_count":4,"outputs":[{"output_type":"stream","text":"Collecting turicreate\n  Downloading turicreate-6.3-cp37-cp37m-manylinux1_x86_64.whl (91.9 MB)\n\u001b[K     |████████████████████████████████| 91.9 MB 2.7 kB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from turicreate) (1.14.0)\nRequirement already satisfied: prettytable==0.7.2 in /opt/conda/lib/python3.7/site-packages (from turicreate) (0.7.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from turicreate) (1.18.5)\nRequirement already satisfied: decorator>=4.0.9 in /opt/conda/lib/python3.7/site-packages (from turicreate) (4.4.2)\nCollecting resampy==0.2.1\n  Downloading resampy-0.2.1.tar.gz (322 kB)\n\u001b[K     |████████████████████████████████| 322 kB 16.9 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: pillow>=5.2.0 in /opt/conda/lib/python3.7/site-packages (from turicreate) (7.2.0)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from turicreate) (1.4.1)\nRequirement already satisfied: requests>=2.9.1 in /opt/conda/lib/python3.7/site-packages (from turicreate) (2.23.0)\nCollecting coremltools==3.3\n  Downloading coremltools-3.3-cp37-none-manylinux1_x86_64.whl (3.5 MB)\n\u001b[K     |████████████████████████████████| 3.5 MB 20.0 MB/s eta 0:00:01\n\u001b[?25hCollecting tensorflow<=2.0.1,>=2.0.0\n  Downloading tensorflow-2.0.1-cp37-cp37m-manylinux2010_x86_64.whl (86.3 MB)\n\u001b[K     |████████████████████████████████| 86.3 MB 85 kB/s s eta 0:00:01��████████▌    | 74.1 MB 17.7 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: pandas>=0.23.2 in /opt/conda/lib/python3.7/site-packages (from turicreate) (1.1.0)\nRequirement already satisfied: numba>=0.32 in /opt/conda/lib/python3.7/site-packages (from resampy==0.2.1->turicreate) (0.48.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.9.1->turicreate) (2.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.9.1->turicreate) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.9.1->turicreate) (1.24.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.9.1->turicreate) (2020.6.20)\nRequirement already satisfied: protobuf>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from coremltools==3.3->turicreate) (3.12.4)\nCollecting keras-applications>=1.0.8\n  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n\u001b[K     |████████████████████████████████| 50 kB 3.2 MB/s  eta 0:00:01\n\u001b[?25hCollecting tensorboard<2.1.0,>=2.0.0\n  Downloading tensorboard-2.0.2-py3-none-any.whl (3.8 MB)\n\u001b[K     |████████████████████████████████| 3.8 MB 8.6 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.0.1,>=2.0.0->turicreate) (1.1.0)\nCollecting astor>=0.6.0\n  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\nCollecting tensorflow-estimator<2.1.0,>=2.0.0\n  Downloading tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449 kB)\n\u001b[K     |████████████████████████████████| 449 kB 23.7 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.0.1,>=2.0.0->turicreate) (1.31.0)\nRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.0.1,>=2.0.0->turicreate) (0.34.2)\nRequirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.0.1,>=2.0.0->turicreate) (0.2.0)\nRequirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.0.1,>=2.0.0->turicreate) (0.9.0)\nRequirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.0.1,>=2.0.0->turicreate) (1.11.2)\nRequirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.0.1,>=2.0.0->turicreate) (1.1.2)\nCollecting gast==0.2.2\n  Downloading gast-0.2.2.tar.gz (10 kB)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<=2.0.1,>=2.0.0->turicreate) (3.3.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.23.2->turicreate) (2.8.1)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.23.2->turicreate) (2019.3)\nRequirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /opt/conda/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.1->turicreate) (0.31.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from numba>=0.32->resampy==0.2.1->turicreate) (46.1.3.post20200325)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow<=2.0.1,>=2.0.0->turicreate) (2.10.0)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<=2.0.1,>=2.0.0->turicreate) (0.4.1)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<=2.0.1,>=2.0.0->turicreate) (1.0.1)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<=2.0.1,>=2.0.0->turicreate) (1.14.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow<=2.0.1,>=2.0.0->turicreate) (3.2.1)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow<=2.0.1,>=2.0.0->turicreate) (1.2.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow<=2.0.1,>=2.0.0->turicreate) (0.2.7)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow<=2.0.1,>=2.0.0->turicreate) (3.1.1)\nRequirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow<=2.0.1,>=2.0.0->turicreate) (4.0)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow<=2.0.1,>=2.0.0->turicreate) (3.0.1)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow<=2.0.1,>=2.0.0->turicreate) (0.4.8)\nBuilding wheels for collected packages: resampy, gast\n  Building wheel for resampy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for resampy: filename=resampy-0.2.1-py3-none-any.whl size=320848 sha256=c6ce0b37c50f5b0b7ff2a328d01b8873b2abf9a1d47c67e5cbcc219b81181a1f\n  Stored in directory: /root/.cache/pip/wheels/71/74/53/d5ceb7c5ee7a168c7d106041863e71ac3273f4a4677743a284\n  Building wheel for gast (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7539 sha256=5541c687b132785e74d58eed55ad5a7b31dd740f277b39c11bde21cf2d6dec00\n  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\nSuccessfully built resampy gast\nInstalling collected packages: resampy, coremltools, keras-applications, tensorboard, astor, tensorflow-estimator, gast, tensorflow, turicreate\n  Attempting uninstall: resampy\n    Found existing installation: resampy 0.2.2\n    Uninstalling resampy-0.2.2:\n      Successfully uninstalled resampy-0.2.2\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.3.0\n    Uninstalling tensorboard-2.3.0:\n      Successfully uninstalled tensorboard-2.3.0\n","name":"stdout"},{"output_type":"stream","text":"  Attempting uninstall: tensorflow-estimator\n    Found existing installation: tensorflow-estimator 2.3.0\n    Uninstalling tensorflow-estimator-2.3.0:\n      Successfully uninstalled tensorflow-estimator-2.3.0\n  Attempting uninstall: gast\n    Found existing installation: gast 0.3.3\n    Uninstalling gast-0.3.3:\n      Successfully uninstalled gast-0.3.3\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.3.0\n    Uninstalling tensorflow-2.3.0:\n      Successfully uninstalled tensorflow-2.3.0\n\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n\nWe recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n\ntensorflow-probability 0.11.0 requires gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\nlibrosa 0.8.0 requires resampy>=0.2.2, but you'll have resampy 0.2.1 which is incompatible.\u001b[0m\nSuccessfully installed astor-0.8.1 coremltools-3.3 gast-0.2.2 keras-applications-1.0.8 resampy-0.2.1 tensorboard-2.0.2 tensorflow-2.0.1 tensorflow-estimator-2.0.1 turicreate-6.3\n\u001b[33mWARNING: You are using pip version 20.2.1; however, version 20.2.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales= tc.SFrame('../input/house-data')","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LASSO (coordinate descent)\nWe will implement your very own LASSO solver via coordinate descent. \n\n##Write a function to normalize features\n##Implement coordinate descent for LASSO\n##Explore effects of L1 penalty\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# In the dataset, 'floors' was defined with type string, \n# so we'll convert them to int, before using it below\nsales['floors'] = sales['floors'].astype(int) ","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # note this allows us to refer to numpy as np instead \n","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_numpy_data(data_sframe, features, output):\n    data_sframe['constant'] = 1 # this is how you add a constant column to an SFrame\n    # add the column 'constant' to the front of the features list so that we can extract it along with the others:\n    features = ['constant'] + features # this is how you combine two lists\n    # select the columns of data_SFrame given by the features list into the SFrame features_sframe (now including constant):\n    features_sframe = data_sframe[features]\n    # the following line will convert the features_SFrame into a numpy matrix:\n    feature_matrix = features_sframe.to_numpy()\n    # assign the column of data_sframe associated with the output to the SArray output_sarray\n    output_sarray = data_sframe[output]\n    # the following will convert the SArray into a numpy array by first converting it to a list\n    output_array = output_sarray.to_numpy()\n    return(feature_matrix, output_array)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_output(feature_matrix, weights):\n    # assume feature_matrix is a numpy matrix containing the features as columns and weights is a corresponding numpy array\n    # create the predictions vector by using np.dot()\n    predictions = np.dot(feature_matrix, weights)\n\n    return(predictions)","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Normalize features\nIn the house dataset, features vary wildly in their relative magnitude: sqft_living is very large overall compared to bedrooms, for instance. As a result, weight for sqft_living would be much smaller than weight for bedrooms. This is problematic because \"small\" weights are dropped first as l1_penalty goes up.\n\nTo give equal considerations for all features, we need to normalize features as discussed in the lectures: we divide each feature by its 2-norm so that the transformed feature has norm 1.\n\nLet's see how we can do this normalization easily with Numpy: let us first consider a small matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array([[3.,5.,8.],[4.,12.,15.]])\nprint (X)","execution_count":10,"outputs":[{"output_type":"stream","text":"[[ 3.  5.  8.]\n [ 4. 12. 15.]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"norms = np.linalg.norm(X, axis=0) # gives [norm(X[:,0]), norm(X[:,1]), norm(X[:,2])]\nprint (norms)","execution_count":11,"outputs":[{"output_type":"stream","text":"[ 5. 13. 17.]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (X / norms) # gives [X[:,0]/norm(X[:,0]), X[:,1]/norm(X[:,1]), X[:,2]/norm(X[:,2])]","execution_count":12,"outputs":[{"output_type":"stream","text":"[[0.6        0.38461538 0.47058824]\n [0.8        0.92307692 0.88235294]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize_features(feature_matrix):\n    norms = np.linalg.norm(feature_matrix, axis=0)\n    normalized_features = feature_matrix/norms\n    return (normalized_features, norms)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features, norms = normalize_features(np.array([[3.,6.,9.],[4.,8.,12.]]))\nprint (features)\n# should print\n# [[ 0.6  0.6  0.6]\n#  [ 0.8  0.8  0.8]]\nprint (norms)\n# should print\n# [5.  10.  15.]","execution_count":14,"outputs":[{"output_type":"stream","text":"[[0.6 0.6 0.6]\n [0.8 0.8 0.8]]\n[ 5. 10. 15.]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Implementing Coordinate Descent with normalized features\nWe seek to obtain a sparse set of weights by minimizing the LASSO cost function\n\nSUM[ (prediction - output)^2 ] + lambda*( |w[1]| + ... + |w[k]|).\n(By convention, we do not include w[0] in the L1 penalty term. We never want to push the intercept to zero.)\n\nThe absolute value sign makes the cost function non-differentiable, so simple gradient descent is not viable (you would need to implement a method called subgradient descent). Instead, we will use coordinate descent: at each iteration, we will fix all weights but weight i and find the value of weight i that minimizes the objective. That is, we look for\n\nargmin_{w[i]} [ SUM[ (prediction - output)^2 ] + lambda*( |w[1]| + ... + |w[k]|) ]\nwhere all weights other than w[i] are held to be constant. We will optimize one w[i] at a time, circling through the weights multiple times.\n\n#Pick a coordinate i\n#Compute w[i] that minimizes the cost function SUM[ (prediction - output)^2 ] + lambda*( |w[1]| + ... + |w[k]|)\n#Repeat Steps 1 and 2 for all coordinates, multiple times\nFor this notebook, we use cyclical coordinate descent with normalized features, where we cycle through coordinates 0 to (d-1) in order, and assume the features were normalized as discussed above. The formula for optimizing each coordinate is as follows:\n\n           ┌ (ro[i] + lambda/2)     if ro[i] < -lambda/2\n##w[i] = ├ 0                      if -lambda/2 <= ro[i] <= lambda/2\n           └ (ro[i] - lambda/2)     if ro[i] > lambda/2\n##where\n\n##ro[i] = SUM[ [feature_i]*(output - prediction + w[i]*[feature_i]) ].\nNote that we do not regularize the weight of the constant feature (intercept) w[0], so, for this weight, the update is simply:\n\nw[0] = ro[i]"},{"metadata":{"trusted":true},"cell_type":"code","source":"simple_features = ['sqft_living', 'bedrooms']\nmy_output = 'price'\n(simple_feature_matrix, output) = get_numpy_data(sales, simple_features, my_output)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"simple_feature_matrix, norms = normalize_features(simple_feature_matrix)\n","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights = np.array([1., 4., 1.])\n","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = predict_output(simple_feature_matrix, weights)\n","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ro = [0 for i in range((simple_feature_matrix.shape)[1])]\nfor j in range((simple_feature_matrix.shape)[1]):   \n    ro[j] = (simple_feature_matrix[:,j] * (output - prediction + (weights[j] * simple_feature_matrix[:,j]))).sum()\nprint (ro)","execution_count":19,"outputs":[{"output_type":"stream","text":"[79400300.03492916, 87939470.77299108, 80966698.67596565]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Return True if value is within the threshold ranges otherwise False\n# Looking for range -l1_penalty/2 <= ro <= l1_penalty/2\ndef in_l1range(value, penalty):\n    return ( (value >= -penalty/2.) and (value <= penalty/2.) )","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for l1_penalty in [1.4e8, 1.64e8, 1.73e8, 1.9e8, 2.3e8]:\n    print (in_l1range(ro[1], l1_penalty), in_l1range(ro[2], l1_penalty))","execution_count":22,"outputs":[{"output_type":"stream","text":"False False\nFalse True\nFalse True\nTrue True\nTrue True\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Single Coordinate Descent Step\nUsing the formula above, implement coordinate descent that minimizes the cost function over a single feature i. Note that the intercept (weight 0) is not regularized. The function should accept feature matrix, output, current weights, l1 penalty, and index of feature to optimize over. The function should return new weight for feature i."},{"metadata":{"trusted":true},"cell_type":"code","source":" def lasso_coordinate_descent_step(i, feature_matrix, output, weights, l1_penalty):\n    # compute prediction\n    prediction = predict_output(feature_matrix, weights)\n    # compute ro[i] = SUM[ [feature_i]*(output - prediction + weight[i]*[feature_i]) ]\n    ro_i = (feature_matrix[:,i] * (output - prediction + (weights[i] * feature_matrix[:,i]))).sum()\n\n    if i == 0: # intercept -- do not regularize\n        new_weight_i = ro_i \n    elif ro_i < -l1_penalty/2.:\n        new_weight_i = (ro_i + l1_penalty/2.)\n    elif ro_i > l1_penalty/2.:\n        new_weight_i = (ro_i - l1_penalty/2.)\n    else:\n        new_weight_i = 0.\n    \n    return new_weight_i","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# should print 0.425558846691\nimport math\nprint (lasso_coordinate_descent_step(1, np.array([[3./math.sqrt(13),1./math.sqrt(10)],[2./math.sqrt(13),3./math.sqrt(10)]]), \n                                   np.array([1., 1.]), np.array([1., 4.]), 0.1))","execution_count":25,"outputs":[{"output_type":"stream","text":"0.4255588466910251\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Cyclical coordinate descent\nNow that we have a function that optimizes the cost function over a single coordinate, let us implement cyclical coordinate descent where we optimize coordinates 0, 1, ..., (d-1) in order and repeat.\n\nWhen do we know to stop? Each time we scan all the coordinates (features) once, we measure the change in weight for each coordinate. If no coordinate changes by more than a specified threshold, we stop.\n\nFor each iteration:\n\nAs you loop over features in order and perform coordinate descent, measure how much each coordinate changes.\nAfter the loop, if the maximum change across all coordinates is falls below the tolerance, stop. Otherwise, go back to step 1.\nReturn weights\n\n##IMPORTANT: when computing a new weight for coordinate i, make sure to incorporate the new weights for coordinates 0, 1, ..., i-1. One good way is to update your weights variable in-place. See following pseudocode for illustration.\n\nfor i in range(len(weights)):\n    old_weights_i = weights[i] # remember old value of weight[i], as it will be overwritten\n    # the following line uses new values for weight[0], weight[1], ..., weight[i-1]\n    #     and old values for weight[i], ..., weight[d-1]\n    weights[i] = lasso_coordinate_descent_step(i, feature_matrix, output, weights, l1_penalty)\n\n    # use old_weights_i to compute change in coordinate\n    ..."},{"metadata":{"trusted":true},"cell_type":"code","source":"def lasso_cyclical_coordinate_descent(feature_matrix, output, initial_weights, l1_penalty, tolerance):\n    D = feature_matrix.shape[1]\n    weights = np.array(initial_weights)\n    change = np.array(initial_weights) * 0.0\n    converged = False\n\n    while not converged:\n\n    # Evaluate over all features\n        for idx in range(D):\n#             print 'Feature: ' + str(idx)\n            # new weight for feature\n            new_weight = lasso_coordinate_descent_step(idx, feature_matrix,\n                                                       output, weights,\n                                                       l1_penalty)\n            # compute change in weight for feature\n            change[idx] = np.abs(new_weight - weights[idx])\n#             print '  -> old weight: ' + str(weights[idx]) + ', new weight: ' + str(new_weight)\n#             print '  -> abs change (new - old): ' + str(change[idx])\n#             print '  >> old weights: ', weights\n\n            # assign new weight\n            weights[idx] = new_weight\n#             print '  >> new weights: ', weights\n        # maximum change in weight, after all changes have been computed\n        max_change = max(change)\n#         print '  ** max change: ' + str(max_change)\n#         print '--------------------------------------------------'\n        if max_change < tolerance:\n            converged = True\n    return weights","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"simple_features = ['sqft_living', 'bedrooms']\nmy_output = 'price'\ninitial_weights = np.zeros(3)\nl1_penalty = 1e7\ntolerance = 1.0","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(simple_feature_matrix, output) = get_numpy_data(sales, simple_features, my_output)\n(normalized_simple_feature_matrix, simple_norms) = normalize_features(simple_feature_matrix) # normalize features","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights = lasso_cyclical_coordinate_descent(normalized_simple_feature_matrix, output,\n                                            initial_weights, l1_penalty, tolerance)","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (weights)","execution_count":30,"outputs":[{"output_type":"stream","text":"[21624998.36636292 63157246.78545421        0.        ]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction =  predict_output(normalized_simple_feature_matrix, weights)\nRSS = np.dot(output-prediction, output-prediction)\nprint ('RSS for normalized dataset = ', RSS)","execution_count":32,"outputs":[{"output_type":"stream","text":"RSS for normalized dataset =  1630492481484487.5\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Evaluating LASSO fit with more features\nLet us split the sales dataset into training and test sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data,test_data = sales.random_split(.8,seed=0)","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_features = ['bedrooms',\n                'bathrooms',\n                'sqft_living',\n                'sqft_lot',\n                'floors',\n                'waterfront', \n                'view', \n                'condition', \n                'grade',\n                'sqft_above',\n                'sqft_basement',\n                'yr_built', \n                'yr_renovated']","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_output = 'price'\n(feature_matrix, output) = get_numpy_data(train_data, all_features, my_output)\nnormalized_feature_matrix, norms = normalize_features(feature_matrix)","execution_count":35,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, learn the weights with l1_penalty=1e7, on the training data. Initialize weights to all zeros, and set the tolerance=1. Call resulting weights weights1e7, you will need them later."},{"metadata":{"trusted":true},"cell_type":"code","source":"initial_weights = np.zeros(len(all_features) + 1)\nl1_penalty = 1e7\ntolerance = 1.0","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weights1e7 = lasso_cyclical_coordinate_descent(normalized_feature_matrix, output,\n                                               initial_weights, l1_penalty, tolerance)","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (weights1e7)","execution_count":38,"outputs":[{"output_type":"stream","text":"[24429600.60933314        0.                0.         48389174.35227978\n        0.                0.          3317511.16271982  7329961.9848964\n        0.                0.                0.                0.\n        0.                0.        ]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_list = ['constant'] + all_features\nprint (feature_list)","execution_count":40,"outputs":[{"output_type":"stream","text":"['constant', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_weights1e7 = dict(zip(feature_list, weights1e7))\nfor k,v in feature_weights1e7.items():\n    if v != 0.0:\n        print (k, v)","execution_count":42,"outputs":[{"output_type":"stream","text":"constant 24429600.609333135\nsqft_living 48389174.35227978\nwaterfront 3317511.162719816\nview 7329961.984896399\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Next, learn the weights with l1_penalty=1e8, on the training data. Initialize weights to all zeros, and set the tolerance=1. Call resulting weights weights1e8, you will need them later."},{"metadata":{"trusted":true},"cell_type":"code","source":"l1_penalty=1e8\ntolerance = 1.0\nweights1e8 = lasso_cyclical_coordinate_descent(normalized_feature_matrix, output,\n                                               initial_weights, l1_penalty, tolerance)","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (weights1e8)","execution_count":44,"outputs":[{"output_type":"stream","text":"[71114625.75280938        0.                0.                0.\n        0.                0.                0.                0.\n        0.                0.                0.                0.\n        0.                0.        ]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_weights1e8 = dict(zip(feature_list, weights1e8))\nfor k,v in feature_weights1e8.items():\n    if v != 0.0:\n        print (k, v)","execution_count":45,"outputs":[{"output_type":"stream","text":"constant 71114625.75280938\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Finally, learn the weights with l1_penalty=1e4, on the training data. Initialize weights to all zeros, and set the tolerance=5e5. Call resulting weights weights1e4, you will need them later. (This case will take quite a bit longer to converge than the others above.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"l1_penalty=1e4\ntolerance=5e5\nweights1e4 = lasso_cyclical_coordinate_descent(normalized_feature_matrix, output,\n                                               initial_weights, l1_penalty, tolerance)","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (weights1e4)","execution_count":48,"outputs":[{"output_type":"stream","text":"[ 77779073.91265215 -22884012.25023361  15348487.08089997\n  92166869.69883084  -2139328.0824278   -8818455.54409496\n   6494209.73310655   7065162.05053197   4119079.21006765\n  18436483.52618778 -14566678.54514349  -5528348.75179429\n -83591746.20730527   2784276.46012858]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_weights1e4 = dict(zip(feature_list, weights1e4))\nfor k,v in feature_weights1e4.items():\n    if v != 0.0:\n        print (k, v)","execution_count":49,"outputs":[{"output_type":"stream","text":"constant 77779073.91265215\nbedrooms -22884012.250233613\nbathrooms 15348487.080899969\nsqft_living 92166869.69883084\nsqft_lot -2139328.0824277983\nfloors -8818455.544094957\nwaterfront 6494209.733106553\nview 7065162.050531973\ncondition 4119079.210067645\ngrade 18436483.526187785\nsqft_above -14566678.545143493\nsqft_basement -5528348.75179429\nyr_built -83591746.20730527\nyr_renovated 2784276.460128576\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Rescaling learned weights\nRecall that we normalized our feature matrix, before learning the weights. To use these weights on a test set, we must normalize the test data in the same way.\n\nAlternatively, we can rescale the learned weights to include the normalization, so we never have to worry about normalizing the test data:\n\nIn this case, we must scale the resulting weights so that we can make predictions with original features:\n\nStore the norms of the original features to a vector called norms:\nfeatures, norms = normalize_features(features)\nRun Lasso on the normalized features and obtain a weights vector\nCompute the weights for the original features by performing element-wise division, i.e.\nweights_normalized = weights / norms\nNow, we can apply weights_normalized to the test data, without normalizing it!\nCreate a normalized version of each of the weights learned above. (weights1e4, weights1e7, weights1e8)."},{"metadata":{"trusted":true},"cell_type":"code","source":"my_output = 'price'\n(feature_matrix, output) = get_numpy_data(train_data, all_features, my_output)\nnormalized_feature_matrix, norms = normalize_features(feature_matrix)","execution_count":50,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normalized_weights1e7 = weights1e7 / norms\nnormalized_weights1e8 = weights1e8 / norms\nnormalized_weights1e4 = weights1e4 / norms\nprint (normalized_weights1e7[3])","execution_count":52,"outputs":[{"output_type":"stream","text":"161.3174562483786\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Evaluating each of the learned models on the test data\nLet's now evaluate the three models on the test data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"(test_feature_matrix, test_output) = get_numpy_data(test_data, all_features, 'price')\n","execution_count":53,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction =  predict_output(test_feature_matrix, normalized_weights1e7)\nRSS = np.dot(test_output-prediction, test_output-prediction)\nprint ('RSS for model with weights1e7 = ', RSS)","execution_count":55,"outputs":[{"output_type":"stream","text":"RSS for model with weights1e7 =  275962079909185.3\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction =  predict_output(test_feature_matrix, normalized_weights1e8)\nRSS = np.dot(test_output-prediction, test_output-prediction)\nprint (\"\"'RSS for model with weights1e8 = ', RSS)","execution_count":57,"outputs":[{"output_type":"stream","text":"RSS for model with weights1e8 =  537166150034084.9\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction =  predict_output(test_feature_matrix, normalized_weights1e4)\nRSS = np.dot(test_output-prediction, test_output-prediction)\nprint ('RSS for model with weights1e4 = ', RSS)","execution_count":59,"outputs":[{"output_type":"stream","text":"RSS for model with weights1e4 =  227781004760225.34\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}